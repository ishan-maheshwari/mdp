{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "ToH Q Learning with FrozenLake.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cobFTSBhKO3M",
        "colab_type": "text"
      },
      "source": [
        "# Q* Learning with FrozenLake üïπÔ∏è‚õÑ\n",
        "<br> \n",
        "In this Notebook, we'll implement an agent <b>that plays FrozenLake.</b>\n",
        "<img src=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/frozenlake.png?raw=1\" alt=\"Frozen Lake\"/>\n",
        "\n",
        "The goal of this game is <b>to go from the starting state (S) to the goal state (G)</b> by walking only on frozen tiles (F) and avoid holes (H).However, the ice is slippery, <b>so you won't always move in the direction you intend (stochastic environment)</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmwIrlCnEC1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0e20ed42-9a6d-4a1c-adbc-139a35358a8b"
      },
      "source": [
        "!pip install -e \"git+git://github.com/AlexMatthers/toh-gym#egg=toh-gym\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining toh-gym from git+git://github.com/AlexMatthers/toh-gym#egg=toh-gym\n",
            "  Updating ./src/toh-gym clone\n",
            "  Running command git fetch -q --tags\n",
            "  Running command git reset --hard -q d3c8a1201a9fd33f9888182bcb737dde5ae8fdb1\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from toh-gym) (0.17.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->toh-gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->toh-gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->toh-gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->toh-gym) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->toh-gym) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->toh-gym) (0.16.0)\n",
            "Installing collected packages: toh-gym\n",
            "  Found existing installation: toh-gym 0.0.1\n",
            "    Can't uninstall 'toh-gym'. No files were found to uninstall.\n",
            "  Running setup.py develop for toh-gym\n",
            "Successfully installed toh-gym\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5vko2jSKO3R",
        "colab_type": "text"
      },
      "source": [
        "## Step 0: Import the dependencies üìö\n",
        "We use 3 libraries:\n",
        "- `Numpy` for our Qtable\n",
        "- `OpenAI Gym` for our FrozenLake Environment\n",
        "- `Random` to generate random numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BnMjH5cKO3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcshmRmaMzjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(V):\n",
        "\t# reshape value function\n",
        "\tV_sq = np.reshape(V, (8,8))\n",
        "\n",
        "\t# plot the state-value function\n",
        "\tfig = plt.figure(figsize=(12, 12))\n",
        "\tax = fig.add_subplot(111)\n",
        "\tim = ax.imshow(V_sq, cmap='cool')\n",
        "\tfor (j,i),label in np.ndenumerate(V_sq):\n",
        "\t    ax.text(i, j, np.round(label, 5), ha='center', va='center', fontsize=14)\n",
        "\tplt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n",
        "\tplt.title('State-Value Function')\n",
        "\tplt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVY8Lcl7KO3U",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Create the environment üéÆ\n",
        "- Here we'll create the FrozenLake environment. \n",
        "- OpenAI Gym is a library <b> composed of many environments that we can use to train our agents.</b>\n",
        "- In our case we choose to use Frozen Lake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cjIVz3MKO3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from toh_gym.envs import TohEnv\n",
        "env = TohEnv(poles=3, rings=3, noise=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCvrVa91KO3X",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Create the Q-table and initialize it üóÑÔ∏è\n",
        "- Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the action_size and the state_size\n",
        "- OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHwkDBbaKO3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21d46fc9-73cd-40fe-aa8e-6a69f67ff572"
      },
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "print(action_size)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTDkMKEaKO3a",
        "colab_type": "code",
        "outputId": "8b1049d1-edf0-4180-85ac-45314f0955b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EebK5roKO3d",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Create the hyperparameters ‚öôÔ∏è\n",
        "- Here, we'll specify the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNBZX-A_KO3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_episodes = 1000000        # Total episodes\n",
        "learning_rate = 0.8           # Learning rate\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.00001             # Exponential decay rate for exploration prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWLU4NM2KO3g",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: The Q learning algorithm üß†\n",
        "- Now we implement the Q learning algorithm:\n",
        "<img src=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/qtable_algo.png?raw=1\" alt=\"Q algo\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xKq_LUtKO3h",
        "colab_type": "code",
        "outputId": "f6386fda-4343-43da-b5e7-23924ac016cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    if episode%10000==0:\n",
        "      print(episode, epsilon)\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.010044946379926412\n",
            "10000 0.9057980017908281\n",
            "20000 0.8205515510221848\n",
            "30000 0.7434173726119561\n",
            "40000 0.6736234817769197\n",
            "50000 0.6104713577990616\n",
            "60000 0.5533289529954497\n",
            "70000 0.501624366972584\n",
            "80000 0.45484012285503594\n",
            "90000 0.41250798820294976\n",
            "100000 0.3742042887844056\n",
            "110000 0.33954566830130456\n",
            "120000 0.30818525163068716\n",
            "130000 0.27980917318191384\n",
            "140000 0.25413343562434\n",
            "150000 0.23090106754657594\n",
            "160000 0.20987955160023092\n",
            "170000 0.19085849738813826\n",
            "180000 0.17364753580654638\n",
            "190000 0.15807441376714268\n",
            "200000 0.1439832702302497\n",
            "210000 0.1312330762951534\n",
            "220000 0.1196962237354631\n",
            "230000 0.10925724785309135\n",
            "240000 0.09981167186874647\n",
            "250000 0.09126496128320936\n",
            "260000 0.08353157774429142\n",
            "270000 0.07653412295025505\n",
            "280000 0.07020256402159587\n",
            "290000 0.06447353258844536\n",
            "300000 0.0592896905786266\n",
            "310000 0.05459915635895585\n",
            "320000 0.05035498548641967\n",
            "330000 0.046514700872410594\n",
            "340000 0.0430398676577474\n",
            "350000 0.03989570854368596\n",
            "360000 0.03705075572902438\n",
            "370000 0.03447653596977187\n",
            "380000 0.03214728560935267\n",
            "390000 0.03003969272727163\n",
            "400000 0.028132663825578455\n",
            "410000 0.026407112718050463\n",
            "420000 0.024845769509225728\n",
            "430000 0.023433007751484786\n",
            "440000 0.02215468805031052\n",
            "450000 0.020998016552475504\n",
            "460000 0.019951416900858686\n",
            "470000 0.019004414374372378\n",
            "480000 0.01814753105343299\n",
            "490000 0.0173721909617561\n",
            "500000 0.01667063423510343\n",
            "510000 0.01603583945795327\n",
            "520000 0.015461453390814\n",
            "530000 0.014941727384867876\n",
            "540000 0.014471459847561442\n",
            "550000 0.014045944183318962\n",
            "560000 0.01366092168835194\n",
            "570000 0.013312538928120213\n",
            "580000 0.0129973091708639\n",
            "590000 0.012712077491219992\n",
            "600000 0.012453989194668943\n",
            "610000 0.012220461246792387\n",
            "620000 0.012009156421396532\n",
            "630000 0.011817959908766808\n",
            "640000 0.011644958149941445\n",
            "650000 0.011488419685170228\n",
            "660000 0.011346777824883324\n",
            "670000 0.011218614969735512\n",
            "680000 0.011102648422795451\n",
            "690000 0.010997717551883657\n",
            "700000 0.010902772173575569\n",
            "710000 0.010816862042612828\n",
            "720000 0.010739127341529372\n",
            "730000 0.01066879007530922\n",
            "740000 0.01060514628495087\n",
            "750000 0.010547559002008997\n",
            "760000 0.010495451873600167\n",
            "770000 0.010448303394069455\n",
            "780000 0.010405641685586563\n",
            "790000 0.0103670397754339\n",
            "800000 0.010332111322720109\n",
            "810000 0.01030050675175057\n",
            "820000 0.01027190975335636\n",
            "830000 0.010246034119165762\n",
            "840000 0.0102226208771347\n",
            "850000 0.010201435699667463\n",
            "860000 0.010182266558387373\n",
            "870000 0.010164921602085531\n",
            "880000 0.010149227236609427\n",
            "890000 0.010135026387474315\n",
            "900000 0.010122176927808983\n",
            "910000 0.010110550255902245\n",
            "920000 0.010100030008113802\n",
            "930000 0.01009051089426781\n",
            "940000 0.01008189764387341\n",
            "950000 0.010074104052625646\n",
            "960000 0.01006705211964379\n",
            "970000 0.010060671266812324\n",
            "980000 0.010054897632411434\n",
            "990000 0.01004967343196745\n",
            "Score over time: 0.589446\n",
            "[[0.55102827 0.631872   0.51190527 0.60693081 0.52752723 0.54848018]\n",
            " [0.55281041 0.54355283 0.54578733 0.64189657 0.54722145 0.55613801]\n",
            " [0.67211953 0.53817373 0.63576501 0.62446765 0.62695085 0.57120858]\n",
            " [0.55526295 0.54757385 0.56585026 0.59092055 0.61195211 0.55086753]\n",
            " [0.63872375 0.59203771 0.59080514 0.58705335 0.71205282 0.65212991]\n",
            " [0.62737832 0.55363062 0.58609471 0.57225909 0.58518442 0.72071736]\n",
            " [0.68508587 0.60468684 0.56829675 0.58197802 0.57947111 0.54353843]\n",
            " [0.78560885 0.6382351  0.60749768 0.67430651 0.62380565 0.67597121]\n",
            " [0.63836243 0.80340258 0.61620127 0.66375235 0.69484213 0.65880834]\n",
            " [0.60537591 0.61896082 0.62659771 0.5963341  0.59043109 0.72178757]\n",
            " [0.68451222 0.75269093 0.84436917 0.73232974 0.67926143 0.70015541]\n",
            " [0.71598268 0.59000392 0.61780571 0.59944124 0.62256863 0.59953963]\n",
            " [0.65039205 0.64378097 0.64871214 0.61635597 0.76784658 0.62931082]\n",
            " [0.73616813 0.79922912 0.82937242 0.93481828 0.72365076 0.7957983 ]\n",
            " [0.67494355 0.68037592 0.67419682 0.68467086 0.90229414 0.69233931]\n",
            " [0.56594937 0.61527465 0.56100937 0.67187489 0.56723618 0.60206592]\n",
            " [0.66710495 0.68711597 0.81281266 0.69053818 0.68890182 0.68260126]\n",
            " [0.93510946 1.         0.90991338 0.8992402  0.87378728 0.8906204 ]\n",
            " [0.7118703  0.71223983 0.74563242 0.7246562  0.83967083 0.68934809]\n",
            " [0.546577   0.60606243 0.59933265 0.70429436 0.58364945 0.55414633]\n",
            " [0.55399541 0.53895195 0.75566384 0.54258173 0.55425699 0.62924639]\n",
            " [0.70028286 0.69301656 0.71373922 0.84625479 0.7116945  0.71288595]\n",
            " [0.67899085 0.6814431  0.67825089 0.66022085 0.79118182 0.64003213]\n",
            " [0.94052944 0.90323604 0.93696167 1.         0.89099383 0.91012933]\n",
            " [0.         0.         0.         0.         0.         0.        ]\n",
            " [0.90220898 0.75881731 0.73362881 0.73947415 0.73661049 0.74088696]\n",
            " [0.80830654 0.95       0.78732267 0.78045746 0.87959461 0.78364277]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r4yobY7MY0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_valuemap = np.amax(qtable, axis=1)\n",
        "# print(q_valuemap.reshape((8,8)))\n",
        "# plot_values(q_valuemap)\n",
        "\n",
        "q_policymap = np.argmax(qtable, axis=1)\n",
        "# print('Final Policy: ')\n",
        "# print(q_policymap.reshape((8,8)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J_-jL-bKO3j",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Use our Q-table to play FrozenLake ! üëæ\n",
        "- After 10 000 episodes, our Q-table can be used as a \"cheatsheet\" to play FrozenLake\"\n",
        "- By running this cell you can see our agent playing FrozenLake."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iN5-NM1LmWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_episodes(enviorment, n_episodes, policy, random = False, max_eplength=1000):\n",
        "    \"\"\"\n",
        "    This fucntion plays the given number of episodes given by following a policy or sample randomly from action_space.\n",
        "    \n",
        "    Parameters:\n",
        "        enviorment: openAI GYM object\n",
        "        n_episodes: number of episodes to run\n",
        "        policy: Policy to follow while playing an episode\n",
        "        random: Flag for taking random actions. if True no policy would be followed and action will be taken randomly\n",
        "        \n",
        "    Return:\n",
        "        wins: Total number of wins playing n_episodes\n",
        "        total_reward: Total reward of n_episodes\n",
        "        avg_reward: Average reward of n_episodes\n",
        "    \n",
        "    \"\"\"\n",
        "    # intialize wins and total reward\n",
        "    wins = 0\n",
        "    total_reward = 0\n",
        "    \n",
        "    # loop over number of episodes to play\n",
        "    for episode in range(n_episodes):\n",
        "        print(episode)\n",
        "        # flag to check if the game is finished\n",
        "        terminated = False\n",
        "        \n",
        "        # reset the enviorment every time when playing a new episode\n",
        "        state = enviorment.reset()\n",
        "        eplength = 0\n",
        "        while not terminated:\n",
        "            eplength += 1\n",
        "            # check if the random flag is not true then follow the given policy other wise take random action\n",
        "            if random:\n",
        "                action = enviorment.action_space.sample()\n",
        "            else:\n",
        "                action = policy[state]\n",
        "\n",
        "            # take the next step\n",
        "            next_state, reward,  terminated, info = enviorment.step(action)\n",
        "            \n",
        "            # enviorment.render()\n",
        "            \n",
        "            # accumalate total reward\n",
        "            total_reward += reward\n",
        "            \n",
        "            # change the state\n",
        "            state = next_state\n",
        "            \n",
        "            if eplength == max_eplength:\n",
        "              terminated = True\n",
        "            # if game is over with positive reward then add 1.0 in wins\n",
        "            if terminated and reward == 1.0:\n",
        "                wins += 1\n",
        "                \n",
        "    # calculate average reward\n",
        "    average_reward = total_reward / n_episodes\n",
        "    \n",
        "    return wins, total_reward, average_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJEKrNspNt7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9677c20c-75c4-4d92-ab31-17357dc21185"
      },
      "source": [
        "n_episode = 1000\n",
        "wins, total_reward, avg_reward = play_episodes(env, n_episode, q_policymap, random = False, max_eplength = 5000)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbtD1nBMN3sw",
        "colab_type": "code",
        "outputId": "6461a250-8f2d-455c-aa85-89b07e678216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(f'Total wins with q learning: {wins}')\n",
        "print(f\"Average rewards with q learning: {avg_reward}\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total wins with q learning: 780\n",
            "Average rewards with q learning: 0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqvpbEFlKtyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}